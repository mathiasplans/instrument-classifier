{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the audio files and create a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['predominant', 'genre', 'drum'])\n",
    "\n",
    "# Set your own\n",
    "path = 'IRMAS-TrainingData'\n",
    "\n",
    "dirs = os.scandir(path)\n",
    "for dir in dirs:\n",
    "    if os.path.isfile(dir.path):\n",
    "        continue\n",
    "    \n",
    "    pieces = os.scandir(dir.path)\n",
    "    \n",
    "    for p in pieces:\n",
    "        fp = p.path\n",
    "        name = p.name\n",
    "        name = name.replace('.wav', '')\n",
    "        \n",
    "        m = re.search(r'(\\[([a-zA-Z_]+)\\])+', name)\n",
    "        \n",
    "        labels = m.group()[1:-1].split('][')\n",
    "        \n",
    "        predominant = labels[0]\n",
    "        genre = labels[-1]\n",
    "\n",
    "        drum = None\n",
    "        \n",
    "        if len(labels) > 2:\n",
    "            drum = labels[1]\n",
    "            \n",
    "        df = df.append({'file':fp, 'predominant':predominant, 'genre':genre, 'drum':drum}, ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting features from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymir import AudioFile\n",
    "\n",
    "def getPYMIR(file):\n",
    "    wavData = AudioFile.open(file)\n",
    "\n",
    "    def getFrames(wavData):\n",
    "        return wavData.frames(132300)[0]\n",
    "\n",
    "    def getSpectrum(frames):\n",
    "        return frames.spectrum()\n",
    "\n",
    "    frames = getFrames(wavData)\n",
    "    spectrum = getSpectrum(frames)\n",
    "    \n",
    "    mfcd = {}\n",
    "    for i, f in enumerate(spectrum.mfcc2()):\n",
    "        mfcd[f'MFC{i}'] = f\n",
    "\n",
    "    return {**{\n",
    "        \"ZCR\":frames.zcr(),\n",
    "        \"SCrest\":spectrum.crest(),\n",
    "        \"SCentroid\":spectrum.centroid(),\n",
    "        \"SKurtosis\":spectrum.kurtosis(),\n",
    "        \"SMean\":spectrum.mean(),\n",
    "        \"SRolloff\":spectrum.rolloff(),\n",
    "        \"SVariance\":spectrum.variance(),\n",
    "        \"SSkewness\":spectrum.skewness()\n",
    "    }, **mfcd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyACA\n",
    "import timbral_models\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "\n",
    "def getPYACA(file):\n",
    "    def computeFeatureC2(cPath, cFeatureName, bPlotOutput = False):\n",
    "        # read audio file\n",
    "        [f_s, afAudioData] = pyACA.ToolReadAudio(cPath)\n",
    "\n",
    "        # for debugging\n",
    "        #afAudioData = np.sin(2*np.pi * np.arange(f_s*1)*440./f_s)\n",
    "\n",
    "        # compute feature\n",
    "        [v, t] = pyACA.computeFeature(cFeatureName, afAudioData, f_s, None, 132300, 132300)\n",
    "\n",
    "        # plot feature output\n",
    "        if bPlotOutput:\n",
    "            plt.plot(t, v)\n",
    "\n",
    "        return v\n",
    "    \n",
    "    data, rate = sf.read(file) # load audio (with shape (samples, channels))\n",
    "    meter = pyln.Meter(rate) # create BS.1770 meter\n",
    "    loudness = meter.integrated_loudness(data)\n",
    "    \n",
    "    tpe = computeFeatureC2(file, \"TimePeakEnvelope\")\n",
    "\n",
    "    return {**{\n",
    "        \"SSlope\": computeFeatureC2(file, \"SpectralSlope\")[0],\n",
    "        \"TPE00\": tpe[0][0],\n",
    "        \"TPE01\": tpe[0][1],\n",
    "        \"TPE10\": tpe[1][0],\n",
    "        \"TPE11\": tpe[1][1],\n",
    "        \"Loudness\": loudness,\n",
    "    }, **timbral_models.timbral_extractor(file, verbose=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(file):\n",
    "    return {**getPYMIR(file), **getPYACA(file)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new CSV. This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical = df[(df.genre=='cla') & (df.predominant!='gel')]\n",
    "\n",
    "newdf = pd.DataFrame()\n",
    "for index, row in classical.iterrows():\n",
    "    newdf = newdf.append({'instrument': row.predominant, **getFeature(row.file)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('data.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For NN\n",
    "Neural networks can handle more data. The idea is this: make lots of 1D images (timestreams) for the input (spectral flux, centroid, etc.) with some sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pymir import SpectralFlux\n",
    "\n",
    "def getPYMIRbetter(file, samples):\n",
    "    wavData = AudioFile.open(file)\n",
    "\n",
    "    def getFrames(wavData):\n",
    "        return wavData.frames(int(132300/samples))\n",
    "\n",
    "    def getSpectrum(frames):\n",
    "        return [f.spectrum() for f in frames]\n",
    "\n",
    "    frames = getFrames(wavData)\n",
    "    spectrum = getSpectrum(frames)\n",
    "\n",
    "    return {\n",
    "        \"ZCR\":np.asarray([f.zcr() for f in frames]),\n",
    "        \"SCrest\":np.asarray([float(s.crest()) for s in spectrum]),\n",
    "        \"SCentroid\":np.asarray([float(s.centroid()) for s in spectrum]),\n",
    "        \"SKurtosis\":np.asarray([float(s.kurtosis()) for s in spectrum]),\n",
    "        \"SMean\":np.asarray([float(s.mean()) for s in spectrum]),\n",
    "        \"SRolloff\":np.asarray([float(s.rolloff()) for s in spectrum]),\n",
    "        \"SVariance\":np.asarray([float(s.variance()) for s in spectrum]),\n",
    "        \"SSkewness\":np.asarray([float(s.skewness()) for s in spectrum]),\n",
    "        \"MFCD\": np.asarray([spec.mfcc2()[0] for spec in spectrum]),\n",
    "        \"SFlux\": np.asarray(SpectralFlux.spectralFlux(spectrum, rectify = True))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new CSV. This will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical = df[(df.genre=='cla') & (df.predominant!='gel')]\n",
    "classical\n",
    "\n",
    "# The audio file is split into 30 samples\n",
    "smpls = 30\n",
    "\n",
    "newdf = pd.DataFrame()\n",
    "for index, row in classical.iterrows():\n",
    "    newdf = newdf.append({'instrument': row.predominant, **getPYMIRbetter(row.file, smpls)}, ignore_index=True)\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('data.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
